{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21509,"status":"ok","timestamp":1699606023203,"user":{"displayName":"Ayknes 7","userId":"18009231253426645627"},"user_tz":-60},"id":"EsGXhHQXSmdB","outputId":"fce13927-25e1-4f1b-d4c4-d5ba055ccd4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: safetensors, dill, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n","Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"]}],"source":["!pip install datasets transformers"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5830,"status":"ok","timestamp":1699607702836,"user":{"displayName":"Ayknes 7","userId":"18009231253426645627"},"user_tz":-60},"id":"NoKroK8zz7mq","outputId":"37eea2b5-e4f4-4329-92b1-b3028c4b5194"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab.output import eval_js\n","from google.colab.patches import cv2_imshow\n","from google.colab import drive\n","import os\n","from os import listdir\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification\n","import torch\n","import torchvision\n","from PIL import Image\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","drive.mount('/content/drive',force_remount=True)"]},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","# Define a transformation for input images\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load and preprocess an example image\n","image_path = 'path/to/your/image.jpg'\n","image = Image.open(image_path)\n","input_tensor = transform(image)\n","input_batch = input_tensor.unsqueeze(0)\n","\n","# Make a prediction\n","with torch.no_grad():\n","    output = model(input_batch)\n","\n","# Get the predicted class\n","_, predicted_class = output.max(1)\n","\n","# Print the predicted class label\n","print(f\"Predicted class label: {predicted_class.item()}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pwWLiyNhiLpQ","executionInfo":{"status":"ok","timestamp":1699609264958,"user_tz":-60,"elapsed":10411,"user":{"displayName":"Ayknes 7","userId":"18009231253426645627"}},"outputId":"eb792343-f50d-43b8-defb-648caae66d23"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 275, 183])\n","torch.Size([3, 307, 460])\n","torch.Size([3, 731, 1000])\n","torch.Size([3, 675, 1200])\n","torch.Size([3, 168, 299])\n","torch.Size([3, 600, 1000])\n","torch.Size([4, 670, 670])\n","torch.Size([3, 776, 1200])\n","torch.Size([3, 787, 1400])\n","torch.Size([3, 183, 275])\n","torch.Size([3, 1000, 1500])\n","torch.Size([3, 1400, 1400])\n","torch.Size([3, 1488, 2048])\n","torch.Size([3, 175, 289])\n","torch.Size([3, 408, 612])\n","torch.Size([3, 275, 183])\n","torch.Size([3, 1089, 800])\n","torch.Size([3, 808, 1200])\n","torch.Size([3, 800, 1200])\n","torch.Size([3, 900, 1200])\n","torch.Size([3, 572, 1040])\n","torch.Size([3, 1707, 2560])\n","torch.Size([3, 1200, 1200])\n","torch.Size([3, 183, 275])\n","torch.Size([3, 1400, 1400])\n","torch.Size([3, 420, 630])\n","torch.Size([3, 1024, 819])\n","torch.Size([3, 1050, 1400])\n","torch.Size([3, 1440, 2560])\n","torch.Size([3, 1707, 2560])\n","torch.Size([3, 348, 620])\n","torch.Size([3, 1798, 2697])\n","torch.Size([3, 1200, 1200])\n","torch.Size([3, 800, 1200])\n","torch.Size([3, 450, 600])\n","torch.Size([3, 1200, 1200])\n","torch.Size([3, 675, 1200])\n","torch.Size([3, 450, 300])\n","torch.Size([3, 2099, 3153])\n","torch.Size([3, 1664, 2496])\n","torch.Size([3, 757, 1000])\n","torch.Size([3, 675, 1200])\n","torch.Size([3, 1400, 1400])\n","torch.Size([3, 1460, 2187])\n","torch.Size([3, 194, 260])\n","torch.Size([3, 183, 275])\n","torch.Size([3, 628, 1024])\n","torch.Size([3, 551, 980])\n","torch.Size([4, 400, 620])\n","torch.Size([3, 2550, 1913])\n","torch.Size([3, 1365, 2048])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1276, 1052])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1694, 1136])\n","torch.Size([4, 1366, 1084])\n","torch.Size([4, 1694, 1136])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 986, 990])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1804, 1604])\n","torch.Size([4, 1584, 1270])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1804, 1604])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1446, 1006])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1584, 1416])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1584, 1216])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1462, 1032])\n","torch.Size([4, 1362, 802])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1396, 816])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1584, 1108])\n","torch.Size([4, 1362, 802])\n","torch.Size([4, 1446, 906])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1694, 1222])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1398, 1112])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1442, 1126])\n","torch.Size([4, 1398, 1112])\n","torch.Size([4, 1276, 1052])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1502, 1136])\n","torch.Size([4, 1412, 1122])\n","torch.Size([4, 1502, 1136])\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"elapsed":1069,"status":"error","timestamp":1699609557775,"user":{"displayName":"Ayknes 7","userId":"18009231253426645627"},"user_tz":-60},"id":"LMm7m5WmeUs-","outputId":"3be69de3-9892-45be-eb72-4655d8055c21"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 275, 183])\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-fe4be8de0c24>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Preprocess the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add a batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input tensor should be a float tensor. Got {tensor.dtype}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Input tensor should be a float tensor. Got torch.uint8."]}],"source":["import os\n","import torch\n","import torchvision\n","from transformers import AutoFeatureExtractor, ResNetForImageClassification\n","\n","# Load the feature extractor and model outside the loop\n","model_name = \"microsoft/resnet-18\"\n","feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n","model = ResNetForImageClassification.from_pretrained(model_name)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Lists to store uncertain examples\n","uncertain_images = []\n","uncertain_confidences = []\n","folder_dir = '/content/drive/My Drive/Ai TP/dataset_sport/'\n","\n","for picture in os.listdir(folder_dir):\n","    image_path = os.path.join(folder_dir, picture)\n","    image = torchvision.io.read_image(image_path)\n","    print(image.shape)\n","\n","    # Preprocess the image\n","    inputs = torchvision.transforms.functional.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    inputs = inputs.unsqueeze(0)  # Add a batch dimension\n","\n","    with torch.no_grad():\n","        logits = model(inputs).logits\n","\n","    # Apply softmax to get confidence scores\n","    softmax_probs = torch.nn.functional.softmax(logits[0], dim=0)\n","\n","    # Model predicts one of the 1000 ImageNet classes\n","    predicted_label = logits.argmax(-1).item()\n","    confidence = softmax_probs[predicted_label].item() * 100\n","\n","    # Print prediction details\n","    print(f\"Image: {image_path}\")\n","    print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\n","    print(f\"Confidence: {confidence:.2f}%\")\n","\n","    # Check if confidence is below a certain threshold (adjust as needed)\n","    uncertainty_threshold = 70.0\n","    if confidence < uncertainty_threshold:\n","        uncertain_images.append(image_path)\n","        uncertain_confidences.append(confidence)\n","\n","# Print uncertain examples\n","print(\"\\nUncertain Examples:\")\n","for i, image_path in enumerate(uncertain_images):\n","    print(f\"Image {i + 1}: {image_path}\")\n","    print(f\"Confidence: {uncertain_confidences[i]:.2f}%\\n\")\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJcvp+Bw7MOPdZs33Bj5kZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}